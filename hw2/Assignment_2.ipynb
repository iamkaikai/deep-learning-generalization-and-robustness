{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565c2b4f",
   "metadata": {},
   "source": [
    "## Assignment 2: Adversarial Training\n",
    "\n",
    "This assignment requires you to create adversarial examples. You will do adversarial training, i.e., train the model with sets of adversarial examples you generated and evaluate the performances of the model on test sets.\n",
    "\n",
    "### What is Adversarial Training?\n",
    "Adversarial training is a machine learning technique to improve models' robustness by training them on adversarial examples. Adversarial examples are input data that has been intentionally modified to cause the model to misclassify or produce an incorrect output. \n",
    "\n",
    "When a model is trained using adversarial examples, it becomes more resilient to adversarial attacks and is able to better identify and classify input data that may have been modified or corrupted. This may lead to improved performance of the model in real-world scenarios where the input data may not always be perfect.\n",
    "\n",
    "However, adversarial training can also have some negative impacts on ML models. For example, it can lead to overfitting, where the model becomes too specialized to the particular adversarial examples used in training and is unable to generalize well to new examples. Additionally, adversarial training can increase the computational requirements of training the model due to the need for generating adversarial examples.\n",
    "\n",
    "Overall, while adversarial training can improve the robustness of ML models, it is important to carefully consider its potential benefits and drawbacks and to evaluate the trade-offs in terms of model performance and computational requirements. The following are the steps involved in adversarial training:\n",
    "\n",
    "1. Generate adversarial examples: In the first step, we generate adversarial examples by perturbing the original data points in such a way that the modifications are small and not noticeable to humans but are enough to cause misclassification by the neural network.\n",
    "\n",
    "2. Train on adversarial examples: In the second step, we train the neural network on adversarial examples in addition to the original training data. This helps to improve the network's ability to recognize and classify adversarial examples correctly.\n",
    "\n",
    "3. Evaluate performance: In the third step, we evaluate the performance of the network on both the original and adversarial test data. This helps to determine if the adversarial training has improved the network's robustness against adversarial attacks.\n",
    "\n",
    "Overall, adversarial training is a powerful technique that can help improve the security and reliability of deep neural networks.\n",
    "\n",
    "In this Homework, you will run different adversarial training algorithms on the ResNet18 model with the adversarial examples, and evaluating the model performances on test data. The goal is to get experience in generating adversarial examples and train the model with these examples, i.e., adversarial training.\n",
    "\n",
    "We have provided the model architecture ($\\texttt{model.py}$) and some pre-defined functions ($\\texttt{utils.py}$) so you can import and use them directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a91b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from model import ResNet18\n",
    "from utils import trades_loss, mixup_data, mixup_criterion, make_dataloader, eval_test\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7058cd",
   "metadata": {},
   "source": [
    "### Q1 (20 points)\n",
    "Use the following parameters to define the LinfPGDAttack():\n",
    "\n",
    "- Epsilon: 8/255\n",
    "- PGD Steps: 10\n",
    "- PGD Step Size: 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7919023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinfPGDAttack(nn.Module):\n",
    "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.steps = steps\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def perturb(self, x_natural, y):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cross-entropy loss with respect to the input\n",
    "        image `x_adv` and updates the image based on the gradient direction. The \n",
    "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
    "        and is finally clamped to ensure pixel values are valid. \n",
    "        \n",
    "        The resulting perturbed image is returned.\n",
    "        \"\"\"\n",
    "        # *********** Your code starts here ***********\n",
    "        x_adv = x_natural.clone().detach().requires_grad_(True)\n",
    "        for _ in range(self.steps):\n",
    "            self.model.zero_grad()\n",
    "            with torch.enable_grad():  \n",
    "                pred_y = self.model(x_adv)\n",
    "                loss = F.cross_entropy(pred_y, y)\n",
    "            loss.backward()\n",
    "            x_adv = x_adv + self.step_size * x_adv.grad.sign()\n",
    "            x_adv = torch.min(torch.max(x_adv, x_natural - self.epsilon), x_natural + self.epsilon)\n",
    "            x_adv = torch.clamp(x_adv, 0, 1)\n",
    "        \n",
    "            # Prepare x_adv for the next iteration\n",
    "            x_adv = x_adv.detach().requires_grad_(True)\n",
    "\n",
    "        # *********** Your code ends here *************\n",
    "        return x_adv.detach()\n",
    "\n",
    "\n",
    "    def forward(self, x_natural, y):\n",
    "        x_adv = self.perturb(x_natural, y)\n",
    "        return x_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edaaf55",
   "metadata": {},
   "source": [
    "There are many implementations of adversarial training; in this assignment, we ask you to evalaute which training algorithm can make the model more robust to LinfPGDAttack()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835b7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        if mode == 'natural':\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
    "            model.eval()\n",
    "            adv_x = pgd_attack(inputs, targets)\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(adv_x)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
    "            optimizer.zero_grad()\n",
    "            loss = trades_loss(model=model, x_natural=inputs, y=targets, optimizer=optimizer)\n",
    "            \n",
    "        elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
    "            model.eval()\n",
    "            benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)            \n",
    "            adv_x = pgd_attack(inputs, targets)\n",
    "            adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            benign_outputs = model(benign_inputs)\n",
    "            adv_outputs = model(adv_inputs)\n",
    "            loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
    "            loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
    "            \n",
    "            loss = (loss_1 + loss_2) / 2\n",
    "\n",
    "        else:\n",
    "            print(\"No training mode specified.\")\n",
    "            raise ValueError()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
    "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed00bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, pgd_attack,\n",
    "          mode='natural', epochs=25, batch_size=256, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
    "          checkpoint_path='model1.pt'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        # training\n",
    "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
    "\n",
    "        # evaluate clean accuracy\n",
    "        test_loss, test_acc = eval_test(model, val_loader)\n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = test_acc > best_acc\n",
    "        best_acc = max(test_acc, best_acc)\n",
    "\n",
    "        # save checkpoint if is a new best\n",
    "        if is_best:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        print('================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff637f3",
   "metadata": {},
   "source": [
    "### Q2 (40 points)\n",
    "Use the four training modes (\"natural\", \"adv_train\", \"adv_train_trades\", and \"adv_train_mixup\") to obtain four models, and save them as $\\texttt{model1.pt}$, $\\texttt{model2.pt}$, $\\texttt{model3.pt}$, and $\\texttt{model4.pt}$, respectively.\n",
    "\n",
    "When calculating your losses, you may encounter Nan. In this case, you may consider adjusting the `learning rate` to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd505f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1d19e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "batch_size = 256\n",
    "data_path = './dataset/' # directory of the data\n",
    "epsilon = 8/255\n",
    "steps = 10\n",
    "epochs = 1\n",
    "\n",
    "# create data loader\n",
    "train_loader, val_loader = make_dataloader(data_path, batch_size)\n",
    "model = ResNet18()\n",
    "model.to(device)\n",
    "pgd_attack = LinfPGDAttack(model, epsilon, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ce821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [00256/50176 (1%)]\t Loss: 2.361035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8822e3517088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Write your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train(model, train_loader, val_loader, pgd_attack,\n\u001b[0m\u001b[1;32m      8\u001b[0m           \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           checkpoint_path='model1.pt')\n",
      "\u001b[0;32m<ipython-input-5-be91a1bdb337>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, pgd_attack, mode, epochs, batch_size, learning_rate, momentum, weight_decay, checkpoint_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtrain_ep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# evaluate clean accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-cb55771d3551>\u001b[0m in \u001b[0;36mtrain_ep\u001b[0;34m(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_mode = \"natural\"\n",
    "\n",
    "# Define Model and Launch Training (Define Adversary If You Need It)\n",
    "model = ResNet18().to(device)\n",
    "\n",
    "# Write your code here\n",
    "train(model, train_loader, val_loader, pgd_attack,\n",
    "          mode=training_mode, epochs=epochs, batch_size=batch_size, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
    "          checkpoint_path='model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e90e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [00256/50176 (1%)]\t Loss: 2.468390\n",
      "Train Epoch: 0 [13056/50176 (26%)]\t Loss: 2.156428\n",
      "Train Epoch: 0 [25856/50176 (52%)]\t Loss: 1.972983\n",
      "Train Epoch: 0 [38656/50176 (77%)]\t Loss: 1.898687\n",
      "Test: Average loss: 0.0072, Accuracy: 3442/10000 (34%)\n",
      "================================================================\n",
      "Train Epoch: 1 [00256/50176 (1%)]\t Loss: 1.775255\n",
      "Train Epoch: 1 [13056/50176 (26%)]\t Loss: 1.701212\n",
      "Train Epoch: 1 [25856/50176 (52%)]\t Loss: 1.638372\n",
      "Train Epoch: 1 [38656/50176 (77%)]\t Loss: 1.621612\n",
      "Test: Average loss: 0.0063, Accuracy: 4438/10000 (44%)\n",
      "================================================================\n",
      "Train Epoch: 2 [00256/50176 (1%)]\t Loss: 1.496722\n",
      "Train Epoch: 2 [13056/50176 (26%)]\t Loss: 1.401162\n",
      "Train Epoch: 2 [25856/50176 (52%)]\t Loss: 1.386252\n",
      "Train Epoch: 2 [38656/50176 (77%)]\t Loss: 1.216043\n",
      "Test: Average loss: 0.0076, Accuracy: 3937/10000 (39%)\n",
      "================================================================\n",
      "Train Epoch: 3 [00256/50176 (1%)]\t Loss: 1.241223\n",
      "Train Epoch: 3 [13056/50176 (26%)]\t Loss: 1.252075\n",
      "Train Epoch: 3 [25856/50176 (52%)]\t Loss: 0.969843\n",
      "Train Epoch: 3 [38656/50176 (77%)]\t Loss: 1.045187\n",
      "Test: Average loss: 0.0077, Accuracy: 4038/10000 (40%)\n",
      "================================================================\n",
      "Train Epoch: 4 [00256/50176 (1%)]\t Loss: 0.984354\n",
      "Train Epoch: 4 [13056/50176 (26%)]\t Loss: 1.156512\n",
      "Train Epoch: 4 [25856/50176 (52%)]\t Loss: 0.991881\n",
      "Train Epoch: 4 [38656/50176 (77%)]\t Loss: 0.927646\n",
      "Test: Average loss: 0.0047, Accuracy: 5898/10000 (59%)\n",
      "================================================================\n",
      "Train Epoch: 5 [00256/50176 (1%)]\t Loss: 0.862236\n",
      "Train Epoch: 5 [13056/50176 (26%)]\t Loss: 0.919570\n",
      "Train Epoch: 5 [25856/50176 (52%)]\t Loss: 0.846222\n",
      "Train Epoch: 5 [38656/50176 (77%)]\t Loss: 0.855827\n",
      "Test: Average loss: 0.0040, Accuracy: 6504/10000 (65%)\n",
      "================================================================\n",
      "Train Epoch: 6 [00256/50176 (1%)]\t Loss: 0.716301\n",
      "Train Epoch: 6 [13056/50176 (26%)]\t Loss: 0.794125\n",
      "Train Epoch: 6 [25856/50176 (52%)]\t Loss: 0.884776\n",
      "Train Epoch: 6 [38656/50176 (77%)]\t Loss: 0.835461\n",
      "Test: Average loss: 0.0044, Accuracy: 6246/10000 (62%)\n",
      "================================================================\n",
      "Train Epoch: 7 [00256/50176 (1%)]\t Loss: 0.851839\n",
      "Train Epoch: 7 [13056/50176 (26%)]\t Loss: 0.783645\n",
      "Train Epoch: 7 [25856/50176 (52%)]\t Loss: 0.606754\n",
      "Train Epoch: 7 [38656/50176 (77%)]\t Loss: 0.696146\n",
      "Test: Average loss: 0.0037, Accuracy: 6942/10000 (69%)\n",
      "================================================================\n",
      "Train Epoch: 8 [00256/50176 (1%)]\t Loss: 0.641127\n",
      "Train Epoch: 8 [13056/50176 (26%)]\t Loss: 0.685382\n",
      "Train Epoch: 8 [25856/50176 (52%)]\t Loss: 0.806490\n",
      "Train Epoch: 8 [38656/50176 (77%)]\t Loss: 0.598568\n",
      "Test: Average loss: 0.0034, Accuracy: 7212/10000 (72%)\n",
      "================================================================\n",
      "Train Epoch: 9 [00256/50176 (1%)]\t Loss: 0.606569\n",
      "Train Epoch: 9 [13056/50176 (26%)]\t Loss: 0.667730\n",
      "Train Epoch: 9 [25856/50176 (52%)]\t Loss: 0.632338\n",
      "Train Epoch: 9 [38656/50176 (77%)]\t Loss: 0.489692\n",
      "Test: Average loss: 0.0028, Accuracy: 7633/10000 (76%)\n",
      "================================================================\n",
      "Train Epoch: 10 [00256/50176 (1%)]\t Loss: 0.411907\n",
      "Train Epoch: 10 [13056/50176 (26%)]\t Loss: 0.572620\n",
      "Train Epoch: 10 [25856/50176 (52%)]\t Loss: 0.564081\n",
      "Train Epoch: 10 [38656/50176 (77%)]\t Loss: 0.429291\n",
      "Test: Average loss: 0.0023, Accuracy: 7978/10000 (80%)\n",
      "================================================================\n",
      "Train Epoch: 11 [00256/50176 (1%)]\t Loss: 0.533027\n",
      "Train Epoch: 11 [13056/50176 (26%)]\t Loss: 0.417179\n",
      "Train Epoch: 11 [25856/50176 (52%)]\t Loss: 0.483763\n",
      "Train Epoch: 11 [38656/50176 (77%)]\t Loss: 0.483359\n",
      "Test: Average loss: 0.0021, Accuracy: 8189/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 12 [00256/50176 (1%)]\t Loss: 0.418383\n",
      "Train Epoch: 12 [13056/50176 (26%)]\t Loss: 0.487542\n",
      "Train Epoch: 12 [25856/50176 (52%)]\t Loss: 0.452954\n",
      "Train Epoch: 12 [38656/50176 (77%)]\t Loss: 0.402109\n",
      "Test: Average loss: 0.0025, Accuracy: 8009/10000 (80%)\n",
      "================================================================\n",
      "Train Epoch: 13 [00256/50176 (1%)]\t Loss: 0.321238\n",
      "Train Epoch: 13 [13056/50176 (26%)]\t Loss: 0.592068\n",
      "Train Epoch: 13 [25856/50176 (52%)]\t Loss: 0.495446\n",
      "Train Epoch: 13 [38656/50176 (77%)]\t Loss: 0.351435\n",
      "Test: Average loss: 0.0020, Accuracy: 8287/10000 (83%)\n",
      "================================================================\n",
      "Train Epoch: 14 [00256/50176 (1%)]\t Loss: 0.416393\n",
      "Train Epoch: 14 [13056/50176 (26%)]\t Loss: 0.395377\n",
      "Train Epoch: 14 [25856/50176 (52%)]\t Loss: 0.370392\n",
      "Train Epoch: 14 [38656/50176 (77%)]\t Loss: 0.268384\n",
      "Test: Average loss: 0.0021, Accuracy: 8217/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 15 [00256/50176 (1%)]\t Loss: 0.374223\n",
      "Train Epoch: 15 [13056/50176 (26%)]\t Loss: 0.302532\n",
      "Train Epoch: 15 [25856/50176 (52%)]\t Loss: 0.447013\n",
      "Train Epoch: 15 [38656/50176 (77%)]\t Loss: 0.316109\n",
      "Test: Average loss: 0.0024, Accuracy: 8114/10000 (81%)\n",
      "================================================================\n",
      "Train Epoch: 16 [00256/50176 (1%)]\t Loss: 0.303566\n",
      "Train Epoch: 16 [13056/50176 (26%)]\t Loss: 0.379770\n",
      "Train Epoch: 16 [25856/50176 (52%)]\t Loss: 0.409817\n",
      "Train Epoch: 16 [38656/50176 (77%)]\t Loss: 0.402481\n",
      "Test: Average loss: 0.0018, Accuracy: 8507/10000 (85%)\n",
      "================================================================\n",
      "Train Epoch: 17 [00256/50176 (1%)]\t Loss: 0.318104\n",
      "Train Epoch: 17 [13056/50176 (26%)]\t Loss: 0.296901\n",
      "Train Epoch: 17 [25856/50176 (52%)]\t Loss: 0.264656\n",
      "Train Epoch: 17 [38656/50176 (77%)]\t Loss: 0.367709\n",
      "Test: Average loss: 0.0023, Accuracy: 8162/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 18 [00256/50176 (1%)]\t Loss: 0.269347\n",
      "Train Epoch: 18 [13056/50176 (26%)]\t Loss: 0.378409\n",
      "Train Epoch: 18 [25856/50176 (52%)]\t Loss: 0.217964\n",
      "Train Epoch: 18 [38656/50176 (77%)]\t Loss: 0.395366\n",
      "Test: Average loss: 0.0016, Accuracy: 8640/10000 (86%)\n",
      "================================================================\n",
      "Train Epoch: 19 [00256/50176 (1%)]\t Loss: 0.306818\n",
      "Train Epoch: 19 [13056/50176 (26%)]\t Loss: 0.269631\n",
      "Train Epoch: 19 [25856/50176 (52%)]\t Loss: 0.325554\n",
      "Train Epoch: 19 [38656/50176 (77%)]\t Loss: 0.321024\n",
      "Test: Average loss: 0.0019, Accuracy: 8487/10000 (85%)\n",
      "================================================================\n",
      "Train Epoch: 20 [00256/50176 (1%)]\t Loss: 0.315072\n",
      "Train Epoch: 20 [13056/50176 (26%)]\t Loss: 0.266780\n",
      "Train Epoch: 20 [25856/50176 (52%)]\t Loss: 0.280173\n",
      "Train Epoch: 20 [38656/50176 (77%)]\t Loss: 0.259989\n",
      "Test: Average loss: 0.0017, Accuracy: 8595/10000 (86%)\n",
      "================================================================\n",
      "Train Epoch: 21 [00256/50176 (1%)]\t Loss: 0.275008\n",
      "Train Epoch: 21 [13056/50176 (26%)]\t Loss: 0.362579\n",
      "Train Epoch: 21 [25856/50176 (52%)]\t Loss: 0.218073\n",
      "Train Epoch: 21 [38656/50176 (77%)]\t Loss: 0.330099\n",
      "Test: Average loss: 0.0017, Accuracy: 8544/10000 (85%)\n",
      "================================================================\n",
      "Train Epoch: 22 [00256/50176 (1%)]\t Loss: 0.250172\n",
      "Train Epoch: 22 [13056/50176 (26%)]\t Loss: 0.174548\n",
      "Train Epoch: 22 [25856/50176 (52%)]\t Loss: 0.254130\n",
      "Train Epoch: 22 [38656/50176 (77%)]\t Loss: 0.322591\n",
      "Test: Average loss: 0.0018, Accuracy: 8561/10000 (86%)\n",
      "================================================================\n",
      "Train Epoch: 23 [00256/50176 (1%)]\t Loss: 0.145239\n",
      "Train Epoch: 23 [13056/50176 (26%)]\t Loss: 0.248516\n",
      "Train Epoch: 23 [25856/50176 (52%)]\t Loss: 0.272081\n",
      "Train Epoch: 23 [38656/50176 (77%)]\t Loss: 0.257226\n",
      "Test: Average loss: 0.0016, Accuracy: 8693/10000 (87%)\n",
      "================================================================\n",
      "Train Epoch: 24 [00256/50176 (1%)]\t Loss: 0.207415\n",
      "Train Epoch: 24 [13056/50176 (26%)]\t Loss: 0.208720\n",
      "Train Epoch: 24 [25856/50176 (52%)]\t Loss: 0.232145\n",
      "Train Epoch: 24 [38656/50176 (77%)]\t Loss: 0.205792\n",
      "Test: Average loss: 0.0018, Accuracy: 8610/10000 (86%)\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "training_mode = \"adv_train\"\n",
    "\n",
    "# Define Model and Launch Training (Define Adversary If You Need It)\n",
    "model = ResNet18().to(device)\n",
    "\n",
    "# Write your code here\n",
    "train(model, train_loader, val_loader, pgd_attack,\n",
    "          mode=training_mode, epochs=epochs, batch_size=batch_size, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
    "          checkpoint_path='model2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a04ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [00256/50176 (1%)]\t Loss: 2.453964\n",
      "Train Epoch: 0 [13056/50176 (26%)]\t Loss: 1.896858\n",
      "Train Epoch: 0 [25856/50176 (52%)]\t Loss: 1.826412\n",
      "Train Epoch: 0 [38656/50176 (77%)]\t Loss: 1.668022\n",
      "Test: Average loss: 0.0089, Accuracy: 3135/10000 (31%)\n",
      "================================================================\n",
      "Train Epoch: 1 [00256/50176 (1%)]\t Loss: 1.484552\n",
      "Train Epoch: 1 [13056/50176 (26%)]\t Loss: 1.525583\n",
      "Train Epoch: 1 [25856/50176 (52%)]\t Loss: 1.541090\n",
      "Train Epoch: 1 [38656/50176 (77%)]\t Loss: 1.391539\n",
      "Test: Average loss: 0.0050, Accuracy: 5566/10000 (56%)\n",
      "================================================================\n",
      "Train Epoch: 2 [00256/50176 (1%)]\t Loss: 1.392904\n",
      "Train Epoch: 2 [13056/50176 (26%)]\t Loss: 1.228774\n",
      "Train Epoch: 2 [25856/50176 (52%)]\t Loss: 1.252674\n",
      "Train Epoch: 2 [38656/50176 (77%)]\t Loss: 1.270619\n",
      "Test: Average loss: 0.0043, Accuracy: 6313/10000 (63%)\n",
      "================================================================\n",
      "Train Epoch: 3 [00256/50176 (1%)]\t Loss: 1.260016\n",
      "Train Epoch: 3 [13056/50176 (26%)]\t Loss: 1.212505\n",
      "Train Epoch: 3 [25856/50176 (52%)]\t Loss: 1.273879\n",
      "Train Epoch: 3 [38656/50176 (77%)]\t Loss: 1.327008\n",
      "Test: Average loss: 0.0043, Accuracy: 6303/10000 (63%)\n",
      "================================================================\n",
      "Train Epoch: 4 [00256/50176 (1%)]\t Loss: 1.197235\n",
      "Train Epoch: 4 [13056/50176 (26%)]\t Loss: 1.187534\n",
      "Train Epoch: 4 [25856/50176 (52%)]\t Loss: 1.141779\n",
      "Train Epoch: 4 [38656/50176 (77%)]\t Loss: 1.048806\n",
      "Test: Average loss: 0.0036, Accuracy: 6918/10000 (69%)\n",
      "================================================================\n",
      "Train Epoch: 5 [00256/50176 (1%)]\t Loss: 1.033896\n",
      "Train Epoch: 5 [13056/50176 (26%)]\t Loss: 1.090545\n",
      "Train Epoch: 5 [25856/50176 (52%)]\t Loss: 0.970399\n",
      "Train Epoch: 5 [38656/50176 (77%)]\t Loss: 1.060664\n",
      "Test: Average loss: 0.0035, Accuracy: 7058/10000 (71%)\n",
      "================================================================\n",
      "Train Epoch: 6 [00256/50176 (1%)]\t Loss: 1.118462\n",
      "Train Epoch: 6 [13056/50176 (26%)]\t Loss: 1.149270\n",
      "Train Epoch: 6 [25856/50176 (52%)]\t Loss: 0.991406\n",
      "Train Epoch: 6 [38656/50176 (77%)]\t Loss: 1.092551\n",
      "Test: Average loss: 0.0032, Accuracy: 7341/10000 (73%)\n",
      "================================================================\n",
      "Train Epoch: 7 [00256/50176 (1%)]\t Loss: 0.901396\n",
      "Train Epoch: 7 [13056/50176 (26%)]\t Loss: 0.997616\n",
      "Train Epoch: 7 [25856/50176 (52%)]\t Loss: 0.997029\n",
      "Train Epoch: 7 [38656/50176 (77%)]\t Loss: 1.091174\n",
      "Test: Average loss: 0.0031, Accuracy: 7543/10000 (75%)\n",
      "================================================================\n",
      "Train Epoch: 8 [00256/50176 (1%)]\t Loss: 0.962087\n",
      "Train Epoch: 8 [13056/50176 (26%)]\t Loss: 0.973354\n",
      "Train Epoch: 8 [25856/50176 (52%)]\t Loss: 1.077522\n",
      "Train Epoch: 8 [38656/50176 (77%)]\t Loss: 0.968993\n",
      "Test: Average loss: 0.0031, Accuracy: 7458/10000 (75%)\n",
      "================================================================\n",
      "Train Epoch: 9 [00256/50176 (1%)]\t Loss: 1.036245\n",
      "Train Epoch: 9 [13056/50176 (26%)]\t Loss: 0.963535\n",
      "Train Epoch: 9 [25856/50176 (52%)]\t Loss: 1.029100\n",
      "Train Epoch: 9 [38656/50176 (77%)]\t Loss: 0.870661\n",
      "Test: Average loss: 0.0029, Accuracy: 7613/10000 (76%)\n",
      "================================================================\n",
      "Train Epoch: 10 [00256/50176 (1%)]\t Loss: 0.921167\n",
      "Train Epoch: 10 [13056/50176 (26%)]\t Loss: 0.840629\n",
      "Train Epoch: 10 [25856/50176 (52%)]\t Loss: 0.887502\n",
      "Train Epoch: 10 [38656/50176 (77%)]\t Loss: 0.905601\n",
      "Test: Average loss: 0.0028, Accuracy: 7804/10000 (78%)\n",
      "================================================================\n",
      "Train Epoch: 11 [00256/50176 (1%)]\t Loss: 0.877230\n",
      "Train Epoch: 11 [13056/50176 (26%)]\t Loss: 0.904333\n",
      "Train Epoch: 11 [25856/50176 (52%)]\t Loss: 0.903250\n",
      "Train Epoch: 11 [38656/50176 (77%)]\t Loss: 0.838812\n",
      "Test: Average loss: 0.0027, Accuracy: 7818/10000 (78%)\n",
      "================================================================\n",
      "Train Epoch: 12 [00256/50176 (1%)]\t Loss: 0.880976\n",
      "Train Epoch: 12 [13056/50176 (26%)]\t Loss: 0.897966\n",
      "Train Epoch: 12 [25856/50176 (52%)]\t Loss: 0.892058\n",
      "Train Epoch: 12 [38656/50176 (77%)]\t Loss: 0.856082\n",
      "Test: Average loss: 0.0025, Accuracy: 7976/10000 (80%)\n",
      "================================================================\n",
      "Train Epoch: 13 [00256/50176 (1%)]\t Loss: 0.829795\n",
      "Train Epoch: 13 [13056/50176 (26%)]\t Loss: 0.822142\n",
      "Train Epoch: 13 [25856/50176 (52%)]\t Loss: 0.828753\n",
      "Train Epoch: 13 [38656/50176 (77%)]\t Loss: 0.876609\n",
      "Test: Average loss: 0.0027, Accuracy: 7907/10000 (79%)\n",
      "================================================================\n",
      "Train Epoch: 14 [00256/50176 (1%)]\t Loss: 0.845965\n",
      "Train Epoch: 14 [13056/50176 (26%)]\t Loss: 0.901798\n",
      "Train Epoch: 14 [25856/50176 (52%)]\t Loss: 0.880084\n",
      "Train Epoch: 14 [38656/50176 (77%)]\t Loss: 0.841480\n",
      "Test: Average loss: 0.0026, Accuracy: 8024/10000 (80%)\n",
      "================================================================\n",
      "Train Epoch: 15 [00256/50176 (1%)]\t Loss: 0.837461\n",
      "Train Epoch: 15 [13056/50176 (26%)]\t Loss: 0.802546\n",
      "Train Epoch: 15 [25856/50176 (52%)]\t Loss: 0.795327\n",
      "Train Epoch: 15 [38656/50176 (77%)]\t Loss: 0.793136\n",
      "Test: Average loss: 0.0027, Accuracy: 7872/10000 (79%)\n",
      "================================================================\n",
      "Train Epoch: 16 [00256/50176 (1%)]\t Loss: 0.831719\n",
      "Train Epoch: 16 [13056/50176 (26%)]\t Loss: 0.815635\n",
      "Train Epoch: 16 [25856/50176 (52%)]\t Loss: 0.769069\n",
      "Train Epoch: 16 [38656/50176 (77%)]\t Loss: 0.756805\n",
      "Test: Average loss: 0.0026, Accuracy: 8063/10000 (81%)\n",
      "================================================================\n",
      "Train Epoch: 17 [00256/50176 (1%)]\t Loss: 0.823159\n",
      "Train Epoch: 17 [13056/50176 (26%)]\t Loss: 0.773279\n",
      "Train Epoch: 17 [25856/50176 (52%)]\t Loss: 0.781576\n",
      "Train Epoch: 17 [38656/50176 (77%)]\t Loss: 0.789048\n",
      "Test: Average loss: 0.0026, Accuracy: 7983/10000 (80%)\n",
      "================================================================\n",
      "Train Epoch: 18 [00256/50176 (1%)]\t Loss: 0.730135\n",
      "Train Epoch: 18 [13056/50176 (26%)]\t Loss: 0.834338\n",
      "Train Epoch: 18 [25856/50176 (52%)]\t Loss: 0.801700\n",
      "Train Epoch: 18 [38656/50176 (77%)]\t Loss: 0.727956\n",
      "Test: Average loss: 0.0024, Accuracy: 8175/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 19 [00256/50176 (1%)]\t Loss: 0.788984\n",
      "Train Epoch: 19 [13056/50176 (26%)]\t Loss: 0.674993\n",
      "Train Epoch: 19 [25856/50176 (52%)]\t Loss: 0.718767\n",
      "Train Epoch: 19 [38656/50176 (77%)]\t Loss: 0.764821\n",
      "Test: Average loss: 0.0024, Accuracy: 8187/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 20 [00256/50176 (1%)]\t Loss: 0.747384\n",
      "Train Epoch: 20 [13056/50176 (26%)]\t Loss: 0.649852\n",
      "Train Epoch: 20 [25856/50176 (52%)]\t Loss: 0.703622\n",
      "Train Epoch: 20 [38656/50176 (77%)]\t Loss: 0.677258\n",
      "Test: Average loss: 0.0025, Accuracy: 8110/10000 (81%)\n",
      "================================================================\n",
      "Train Epoch: 21 [00256/50176 (1%)]\t Loss: 0.824177\n",
      "Train Epoch: 21 [13056/50176 (26%)]\t Loss: 0.667752\n",
      "Train Epoch: 21 [25856/50176 (52%)]\t Loss: 0.702685\n",
      "Train Epoch: 21 [38656/50176 (77%)]\t Loss: 0.724360\n",
      "Test: Average loss: 0.0023, Accuracy: 8327/10000 (83%)\n",
      "================================================================\n",
      "Train Epoch: 22 [00256/50176 (1%)]\t Loss: 0.637707\n",
      "Train Epoch: 22 [13056/50176 (26%)]\t Loss: 0.663692\n",
      "Train Epoch: 22 [25856/50176 (52%)]\t Loss: 0.712708\n",
      "Train Epoch: 22 [38656/50176 (77%)]\t Loss: 0.667338\n",
      "Test: Average loss: 0.0023, Accuracy: 8206/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 23 [00256/50176 (1%)]\t Loss: 0.673791\n",
      "Train Epoch: 23 [13056/50176 (26%)]\t Loss: 0.682261\n",
      "Train Epoch: 23 [25856/50176 (52%)]\t Loss: 0.715549\n",
      "Train Epoch: 23 [38656/50176 (77%)]\t Loss: 0.659981\n",
      "Test: Average loss: 0.0026, Accuracy: 8066/10000 (81%)\n",
      "================================================================\n",
      "Train Epoch: 24 [00256/50176 (1%)]\t Loss: 0.682962\n",
      "Train Epoch: 24 [13056/50176 (26%)]\t Loss: 0.700586\n",
      "Train Epoch: 24 [25856/50176 (52%)]\t Loss: 0.654806\n",
      "Train Epoch: 24 [38656/50176 (77%)]\t Loss: 0.640500\n",
      "Test: Average loss: 0.0025, Accuracy: 8253/10000 (83%)\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "training_mode = \"adv_train_trades\"\n",
    "\n",
    "# Define Model and Launch Training (Define Adversary If You Need It)\n",
    "model = ResNet18().to(device)\n",
    "\n",
    "# Write your code here\n",
    "train(model, train_loader, val_loader, pgd_attack,\n",
    "          mode=training_mode, epochs=epochs, batch_size=batch_size, learning_rate=0.05, momentum=0.9, weight_decay=2e-4,\n",
    "          checkpoint_path='model3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5036a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mode = \"adv_train_mixup\"\n",
    "\n",
    "# Define Model and Launch Training (Define Adversary If You Need It)\n",
    "model = ResNet18().to(device)\n",
    "\n",
    "# Write your code here\n",
    "train(model, train_loader, val_loader, pgd_attack,\n",
    "          mode=training_mode, epochs=epochs, batch_size=batch_size, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
    "          checkpoint_path='model4.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a009caec",
   "metadata": {},
   "source": [
    "### Q3 (20 points)\n",
    "Use eval_robust() to evaluate each model's robustness against LinfPGDAttack()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d8a3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import eval_robust\n",
    "\n",
    "# *********** Your code starts here ***********pgd_attack\n",
    "# model = ResNet18().to(device)\n",
    "# model.load_state_dict(torch.load('model1.pt'))\n",
    "robust_loss, robust_acc = eval_robust(model, val_loader, )\n",
    "print(f'Training Mode: Natural, robust_loss: {robust_loss}, robust_acc: {robust_acc}')\n",
    "# *********** Your code ends here *************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********** Your code starts here ***********\n",
    "model = ResNet18().to(device)\n",
    "model.load_state_dict(torch.load('model2.pt'))\n",
    "robust_loss, robust_acc = eval_robust(model, val_loader, pgd_attack)\n",
    "print(f'Training Mode: adv_train, robust_loss: {robust_loss}, robust_acc: {robust_acc}')\n",
    "# *********** Your code ends here *************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56863557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********** Your code starts here ***********\n",
    "model = ResNet18().to(device)\n",
    "model.load_state_dict(torch.load('model3.pt'))\n",
    "robust_loss, robust_acc = eval_robust(model, val_loader, pgd_attack)\n",
    "print(f'Training Mode: adv_train_trades, robust_loss: {robust_loss}, robust_acc: {robust_acc}')\n",
    "# *********** Your code ends here *************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********** Your code starts here ***********\n",
    "model = ResNet18().to(device)\n",
    "model.load_state_dict(torch.load('model4.pt'))\n",
    "robust_loss, robust_acc = eval_robust(model, val_loader, pgd_attack)\n",
    "print(f'Training Mode: adv_train_mixup, robust_loss: {robust_loss}, robust_acc: {robust_acc}')\n",
    "# *********** Your code ends here *************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60a88c",
   "metadata": {},
   "source": [
    "### Q4 (10 points)\n",
    "Visualize 10 adversarial examples from (with ground truth labels and model's predictions) from the model having best robust accuarcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********** Your code starts here ***********\n",
    "\n",
    "\n",
    "\n",
    "# *********** Your code ends here *************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6dfa2",
   "metadata": {},
   "source": [
    "### Q5 (10 points)\n",
    "Which adversarial training algorithm achieves the best robust accuracy in only 25 training epochs? Why do you think that adversarial training algorithm outperforms others? Do you encounter any difficulties when you implement this assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15618d3",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412124b3",
   "metadata": {},
   "source": [
    "## Submission Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9ffc6",
   "metadata": {},
   "source": [
    "Please submit this  to Canvas, as well as results, as per instructions. \n",
    "Please compress `First_Middle_Last_HW1/` into one zip file with the name `First_Middle_Last_HW1.zip` before uploading it to Canvas. The directory contains your notebook and four model checkpoints. As listed below:\n",
    "\n",
    "- $\\texttt{Assignment_2.ipynb}$: your code\n",
    "- $\\texttt{model1.pt}$: your model checkpoint with *natural training* \n",
    "- $\\texttt{model2.pt}$: your model checkpoint with *adv_train* training\n",
    "- $\\texttt{model3.pt}$: your model checkpoint with *adv_train_trades* training\n",
    "- $\\texttt{model4.pt}$: your model checkpoint with *adv_train_mixup* training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec7b76",
   "metadata": {},
   "source": [
    "## Academic Integrity\n",
    "\n",
    "This homework assignment must be done individually. Sharing code or model specifications is strictly prohibited. Homework discussions are allowed only on Piazza, according to the policy outlined on the course web page: [https://canvas.dartmouth.edu/courses/63219](https://canvas.dartmouth.edu/courses/63219). You are not allowed to search online for auxiliary software, reference models, architecture specifications, or additional data to solve the homework assignment. Your submission must be entirely your own work. That is, the code and the answers that you submit must be created, typed, and documented by you alone, based exclusively on the materials discussed in class, and released with the homework assignment. You can obviously consult the class slides posted in Canvas, your lecture notes, and the textbook. Important: the models you will submit for this homework assignment must be\n",
    "trained exclusively on the specified data provided with this assignment. You can, of course, play with other datasets in your spare time. These rules will be strictly enforced, and any violation will be treated seriously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
