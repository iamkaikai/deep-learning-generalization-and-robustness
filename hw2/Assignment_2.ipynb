{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565c2b4f",
   "metadata": {},
   "source": [
    "## Assignment 2: Adversarial Training\n",
    "\n",
    "This assignment requires you to create adversarial examples. You will do adversarial training, i.e., train the model with sets of adversarial examples you generated and evaluate the performances of the model on test sets.\n",
    "\n",
    "### What is Adversarial Training?\n",
    "Adversarial training is a machine learning technique to improve models' robustness by training them on adversarial examples. Adversarial examples are input data that has been intentionally modified to cause the model to misclassify or produce an incorrect output. \n",
    "\n",
    "When a model is trained using adversarial examples, it becomes more resilient to adversarial attacks and is able to better identify and classify input data that may have been modified or corrupted. This may lead to improved performance of the model in real-world scenarios where the input data may not always be perfect.\n",
    "\n",
    "However, adversarial training can also have some negative impacts on ML models. For example, it can lead to overfitting, where the model becomes too specialized to the particular adversarial examples used in training and is unable to generalize well to new examples. Additionally, adversarial training can increase the computational requirements of training the model due to the need for generating adversarial examples.\n",
    "\n",
    "Overall, while adversarial training can improve the robustness of ML models, it is important to carefully consider its potential benefits and drawbacks and to evaluate the trade-offs in terms of model performance and computational requirements. The following are the steps involved in adversarial training:\n",
    "\n",
    "1. Generate adversarial examples: In the first step, we generate adversarial examples by perturbing the original data points in such a way that the modifications are small and not noticeable to humans but are enough to cause misclassification by the neural network.\n",
    "\n",
    "2. Train on adversarial examples: In the second step, we train the neural network on adversarial examples in addition to the original training data. This helps to improve the network's ability to recognize and classify adversarial examples correctly.\n",
    "\n",
    "3. Evaluate performance: In the third step, we evaluate the performance of the network on both the original and adversarial test data. This helps to determine if the adversarial training has improved the network's robustness against adversarial attacks.\n",
    "\n",
    "Overall, adversarial training is a powerful technique that can help improve the security and reliability of deep neural networks.\n",
    "\n",
    "In this Homework, you will run different adversarial training algorithms on the ResNet18 model with the adversarial examples, and evaluating the model performances on test data. The goal is to get experience in generating adversarial examples and train the model with these examples, i.e., adversarial training.\n",
    "\n",
    "We have provided the model architecture ($\\texttt{model.py}$) and some pre-defined functions ($\\texttt{utils.py}$) so you can import and use them directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a91b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from model import ResNet18\n",
    "from utils import trades_loss, mixup_data, mixup_criterion, make_dataloader, eval_test\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7058cd",
   "metadata": {},
   "source": [
    "### Q1 (20 points)\n",
    "Use the following parameters to define the LinfPGDAttack():\n",
    "\n",
    "- Epsilon: 8/255\n",
    "- PGD Steps: 10\n",
    "- PGD Step Size: 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7919023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinfPGDAttack(nn.Module):\n",
    "    def __init__(self, model, epsilon, steps=10, step_size=0.003):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.steps = steps\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def perturb(self, x_natural, y):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the cross-entropy loss with respect to the input\n",
    "        image `x_adv` and updates the image based on the gradient direction. The \n",
    "        perturbation is clipped to ensure it stays within a specified epsilon range\n",
    "        and is finally clamped to ensure pixel values are valid. \n",
    "        \n",
    "        The resulting perturbed image is returned.\n",
    "        \"\"\"\n",
    "        # *********** Your code starts here ***********\n",
    "        x_natural = x_natural.to(device)\n",
    "        y = y.to(device)\n",
    "        x_adv = x_natural.clone().detach().requires_grad_(True)\n",
    "        for _ in range(self.steps):\n",
    "            self.model.zero_grad()         \n",
    "            pred_y = self.model(x_adv)\n",
    "            loss = F.cross_entropy(pred_y, y)\n",
    "            loss.backward()\n",
    "            x_adv_update = x_adv + self.step_size * x_adv.grad.sign()\n",
    "            x_adv_update = torch.min(torch.max(x_adv_update, x_natural - self.epsilon), x_natural + self.epsilon)\n",
    "            x_adv_update = torch.clamp(x_adv_update, 0, 1)\n",
    "        \n",
    "            # Prepare x_adv for the next iteration\n",
    "            x_adv = x_adv_update.detach().requires_grad_(False)\n",
    "\n",
    "        # *********** Your code ends here *************\n",
    "        return x_adv.detach()\n",
    "\n",
    "\n",
    "    def forward(self, x_natural, y):\n",
    "        x_adv = self.perturb(x_natural, y)\n",
    "        return x_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edaaf55",
   "metadata": {},
   "source": [
    "There are many implementations of adversarial training; in this assignment, we ask you to evalaute which training algorithm can make the model more robust to LinfPGDAttack()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835b7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        if mode == 'natural':\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        elif mode == 'adv_train': # [Ref] https://arxiv.org/abs/1706.06083\n",
    "            model.eval()\n",
    "            adv_x = pgd_attack(inputs, targets)\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(adv_x)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        elif mode == 'adv_train_trades': # [Ref] https://arxiv.org/abs/1901.08573\n",
    "            optimizer.zero_grad()\n",
    "            loss = trades_loss(model=model, x_natural=inputs, y=targets, optimizer=optimizer)\n",
    "            \n",
    "        elif mode == 'adv_train_mixup': # [Ref] https://arxiv.org/abs/1710.09412\n",
    "            model.eval()\n",
    "            benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)            \n",
    "            adv_x = pgd_attack(inputs, targets)\n",
    "            adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv_x, targets)\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            benign_outputs = model(benign_inputs)\n",
    "            adv_outputs = model(adv_inputs)\n",
    "            loss_1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
    "            loss_2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
    "            \n",
    "            loss = (loss_1 + loss_2) / 2\n",
    "\n",
    "        else:\n",
    "            print(\"No training mode specified.\")\n",
    "            raise ValueError()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(inputs), len(train_loader) * batch_size,\n",
    "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed00bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, pgd_attack,\n",
    "          mode='natural', epochs=25, batch_size=256, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
    "          checkpoint_path='model1.pt'):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        # training\n",
    "        train_ep(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\n",
    "\n",
    "        # evaluate clean accuracy\n",
    "        test_loss, test_acc = eval_test(model, val_loader)\n",
    "\n",
    "        # remember best acc@1 and save checkpoint\n",
    "        is_best = test_acc > best_acc\n",
    "        best_acc = max(test_acc, best_acc)\n",
    "\n",
    "        # save checkpoint if is a new best\n",
    "        if is_best:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        print('================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff637f3",
   "metadata": {},
   "source": [
    "### Q2 (40 points)\n",
    "Use the four training modes (\"natural\", \"adv_train\", \"adv_train_trades\", and \"adv_train_mixup\") to obtain four models, and save them as $\\texttt{model1.pt}$, $\\texttt{model2.pt}$, $\\texttt{model3.pt}$, and $\\texttt{model4.pt}$, respectively.\n",
    "\n",
    "When calculating your losses, you may encounter Nan. In this case, you may consider adjusting the `learning rate` to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd505f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1d19e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "batch_size = 256\n",
    "data_path = './dataset/' # directory of the data\n",
    "epsilon = 8/255\n",
    "steps = 10\n",
    "epochs = 1\n",
    "\n",
    "# create data loader\n",
    "train_loader, val_loader = make_dataloader(data_path, batch_size)\n",
    "model = ResNet18().to(device)\n",
    "pgd_attack = LinfPGDAttack(model, epsilon, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7ce821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [00256/50176 (1%)]\t Loss: 2.439559\n",
      "Train Epoch: 0 [13056/50176 (26%)]\t Loss: 2.139166\n",
      "Train Epoch: 0 [25856/50176 (52%)]\t Loss: 1.868093\n",
      "Train Epoch: 0 [38656/50176 (77%)]\t Loss: 1.755927\n",
      "Test: Average loss: 0.0083, Accuracy: 3297/10000 (33%)\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "training_mode = \"natural\"\n",
    "\n",
    "# Define Model and Launch Training (Define Adversary If You Need It)\n",
    "model = ResNet18().to(device)\n",
    "\n",
    "# Write your code here\n",
    "train(model, train_loader, val_loader, pgd_attack,\n",
    "          mode=training_mode, epochs=epochs, batch_size=batch_size, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
    "          checkpoint_path='model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e90e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [00256/50176 (1%)]\t Loss: 2.468390\n",
      "Train Epoch: 0 [13056/50176 (26%)]\t Loss: 2.156428\n",
      "Train Epoch: 0 [25856/50176 (52%)]\t Loss: 1.972983\n",
      "Train Epoch: 0 [38656/50176 (77%)]\t Loss: 1.898687\n",
      "Test: Average loss: 0.0072, Accuracy: 3442/10000 (34%)\n",
      "================================================================\n",
      "Train Epoch: 1 [00256/50176 (1%)]\t Loss: 1.775255\n",
      "Train Epoch: 1 [13056/50176 (26%)]\t Loss: 1.701212\n",
      "Train Epoch: 1 [25856/50176 (52%)]\t Loss: 1.638372\n",
      "Train Epoch: 1 [38656/50176 (77%)]\t Loss: 1.621612\n",
      "Test: Average loss: 0.0063, Accuracy: 4438/10000 (44%)\n",
      "================================================================\n",
      "Train Epoch: 2 [00256/50176 (1%)]\t Loss: 1.496722\n",
      "Train Epoch: 2 [13056/50176 (26%)]\t Loss: 1.401162\n",
      "Train Epoch: 2 [25856/50176 (52%)]\t Loss: 1.386252\n",
      "Train Epoch: 2 [38656/50176 (77%)]\t Loss: 1.216043\n",
      "Test: Average loss: 0.0076, Accuracy: 3937/10000 (39%)\n",
      "================================================================\n",
      "Train Epoch: 3 [00256/50176 (1%)]\t Loss: 1.241223\n",
      "Train Epoch: 3 [13056/50176 (26%)]\t Loss: 1.252075\n",
      "Train Epoch: 3 [25856/50176 (52%)]\t Loss: 0.969843\n",
      "Train Epoch: 3 [38656/50176 (77%)]\t Loss: 1.045187\n",
      "Test: Average loss: 0.0077, Accuracy: 4038/10000 (40%)\n",
      "================================================================\n",
      "Train Epoch: 4 [00256/50176 (1%)]\t Loss: 0.984354\n",
      "Train Epoch: 4 [13056/50176 (26%)]\t Loss: 1.156512\n",
      "Train Epoch: 4 [25856/50176 (52%)]\t Loss: 0.991881\n",
      "Train Epoch: 4 [38656/50176 (77%)]\t Loss: 0.927646\n",
      "Test: Average loss: 0.0047, Accuracy: 5898/10000 (59%)\n",
      "================================================================\n",
      "Train Epoch: 5 [00256/50176 (1%)]\t Loss: 0.862236\n",
      "Train Epoch: 5 [13056/50176 (26%)]\t Loss: 0.919570\n",
      "Train Epoch: 5 [25856/50176 (52%)]\t Loss: 0.846222\n",
      "Train Epoch: 5 [38656/50176 (77%)]\t Loss: 0.855827\n",
      "Test: Average loss: 0.0040, Accuracy: 6504/10000 (65%)\n",
      "================================================================\n",
      "Train Epoch: 6 [00256/50176 (1%)]\t Loss: 0.716301\n",
      "Train Epoch: 6 [13056/50176 (26%)]\t Loss: 0.794125\n",
      "Train Epoch: 6 [25856/50176 (52%)]\t Loss: 0.884776\n",
      "Train Epoch: 6 [38656/50176 (77%)]\t Loss: 0.835461\n",
      "Test: Average loss: 0.0044, Accuracy: 6246/10000 (62%)\n",
      "================================================================\n",
      "Train Epoch: 7 [00256/50176 (1%)]\t Loss: 0.851839\n",
      "Train Epoch: 7 [13056/50176 (26%)]\t Loss: 0.783645\n",
      "Train Epoch: 7 [25856/50176 (52%)]\t Loss: 0.606754\n",
      "Train Epoch: 7 [38656/50176 (77%)]\t Loss: 0.696146\n",
      "Test: Average loss: 0.0037, Accuracy: 6942/10000 (69%)\n",
      "================================================================\n",
      "Train Epoch: 8 [00256/50176 (1%)]\t Loss: 0.641127\n",
      "Train Epoch: 8 [13056/50176 (26%)]\t Loss: 0.685382\n",
      "Train Epoch: 8 [25856/50176 (52%)]\t Loss: 0.806490\n",
      "Train Epoch: 8 [38656/50176 (77%)]\t Loss: 0.598568\n",
      "Test: Average loss: 0.0034, Accuracy: 7212/10000 (72%)\n",
      "================================================================\n",
      "Train Epoch: 9 [00256/50176 (1%)]\t Loss: 0.606569\n",
      "Train Epoch: 9 [13056/50176 (26%)]\t Loss: 0.667730\n",
      "Train Epoch: 9 [25856/50176 (52%)]\t Loss: 0.632338\n",
      "Train Epoch: 9 [38656/50176 (77%)]\t Loss: 0.489692\n",
      "Test: Average loss: 0.0028, Accuracy: 7633/10000 (76%)\n",
      "================================================================\n",
      "Train Epoch: 10 [00256/50176 (1%)]\t Loss: 0.411907\n",
      "Train Epoch: 10 [13056/50176 (26%)]\t Loss: 0.572620\n",
      "Train Epoch: 10 [25856/50176 (52%)]\t Loss: 0.564081\n",
      "Train Epoch: 10 [38656/50176 (77%)]\t Loss: 0.429291\n",
      "Test: Average loss: 0.0023, Accuracy: 7978/10000 (80%)\n",
      "================================================================\n",
      "Train Epoch: 11 [00256/50176 (1%)]\t Loss: 0.533027\n",
      "Train Epoch: 11 [13056/50176 (26%)]\t Loss: 0.417179\n",
      "Train Epoch: 11 [25856/50176 (52%)]\t Loss: 0.483763\n",
      "Train Epoch: 11 [38656/50176 (77%)]\t Loss: 0.483359\n",
      "Test: Average loss: 0.0021, Accuracy: 8189/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 12 [00256/50176 (1%)]\t Loss: 0.418383\n",
      "Train Epoch: 12 [13056/50176 (26%)]\t Loss: 0.487542\n",
      "Train Epoch: 12 [25856/50176 (52%)]\t Loss: 0.452954\n",
      "Train Epoch: 12 [38656/50176 (77%)]\t Loss: 0.402109\n",
      "Test: Average loss: 0.0025, Accuracy: 8009/10000 (80%)\n",
      "================================================================\n",
      "Train Epoch: 13 [00256/50176 (1%)]\t Loss: 0.321238\n",
      "Train Epoch: 13 [13056/50176 (26%)]\t Loss: 0.592068\n",
      "Train Epoch: 13 [25856/50176 (52%)]\t Loss: 0.495446\n",
      "Train Epoch: 13 [38656/50176 (77%)]\t Loss: 0.351435\n",
      "Test: Average loss: 0.0020, Accuracy: 8287/10000 (83%)\n",
      "================================================================\n",
      "Train Epoch: 14 [00256/50176 (1%)]\t Loss: 0.416393\n",
      "Train Epoch: 14 [13056/50176 (26%)]\t Loss: 0.395377\n",
      "Train Epoch: 14 [25856/50176 (52%)]\t Loss: 0.370392\n",
      "Train Epoch: 14 [38656/50176 (77%)]\t Loss: 0.268384\n",
      "Test: Average loss: 0.0021, Accuracy: 8217/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 15 [00256/50176 (1%)]\t Loss: 0.374223\n",
      "Train Epoch: 15 [13056/50176 (26%)]\t Loss: 0.302532\n",
      "Train Epoch: 15 [25856/50176 (52%)]\t Loss: 0.447013\n",
      "Train Epoch: 15 [38656/50176 (77%)]\t Loss: 0.316109\n",
      "Test: Average loss: 0.0024, Accuracy: 8114/10000 (81%)\n",
      "================================================================\n",
      "Train Epoch: 16 [00256/50176 (1%)]\t Loss: 0.303566\n",
      "Train Epoch: 16 [13056/50176 (26%)]\t Loss: 0.379770\n",
      "Train Epoch: 16 [25856/50176 (52%)]\t Loss: 0.409817\n",
      "Train Epoch: 16 [38656/50176 (77%)]\t Loss: 0.402481\n",
      "Test: Average loss: 0.0018, Accuracy: 8507/10000 (85%)\n",
      "================================================================\n",
      "Train Epoch: 17 [00256/50176 (1%)]\t Loss: 0.318104\n",
      "Train Epoch: 17 [13056/50176 (26%)]\t Loss: 0.296901\n",
      "Train Epoch: 17 [25856/50176 (52%)]\t Loss: 0.264656\n",
      "Train Epoch: 17 [38656/50176 (77%)]\t Loss: 0.367709\n",
      "Test: Average loss: 0.0023, Accuracy: 8162/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 18 [00256/50176 (1%)]\t Loss: 0.269347\n",
      "Train Epoch: 18 [13056/50176 (26%)]\t Loss: 0.378409\n",
      "Train Epoch: 18 [25856/50176 (52%)]\t Loss: 0.217964\n",
      "Train Epoch: 18 [38656/50176 (77%)]\t Loss: 0.395366\n",
      "Test: Average loss: 0.0016, Accuracy: 8640/10000 (86%)\n",
      "================================================================\n",
      "Train Epoch: 19 [00256/50176 (1%)]\t Loss: 0.306818\n",
      "Train Epoch: 19 [13056/50176 (26%)]\t Loss: 0.269631\n",
      "Train Epoch: 19 [25856/50176 (52%)]\t Loss: 0.325554\n",
      "Train Epoch: 19 [38656/50176 (77%)]\t Loss: 0.321024\n",
      "Test: Average loss: 0.0019, Accuracy: 8487/10000 (85%)\n",
      "================================================================\n",
      "Train Epoch: 20 [00256/50176 (1%)]\t Loss: 0.315072\n",
      "Train Epoch: 20 [13056/50176 (26%)]\t Loss: 0.266780\n",
      "Train Epoch: 20 [25856/50176 (52%)]\t Loss: 0.280173\n",
      "Train Epoch: 20 [38656/50176 (77%)]\t Loss: 0.259989\n",
      "Test: Average loss: 0.0017, Accuracy: 8595/10000 (86%)\n",
      "================================================================\n",
      "Train Epoch: 21 [00256/50176 (1%)]\t Loss: 0.275008\n",
      "Train Epoch: 21 [13056/50176 (26%)]\t Loss: 0.362579\n",
      "Train Epoch: 21 [25856/50176 (52%)]\t Loss: 0.218073\n",
      "Train Epoch: 21 [38656/50176 (77%)]\t Loss: 0.330099\n",
      "Test: Average loss: 0.0017, Accuracy: 8544/10000 (85%)\n",
      "================================================================\n",
      "Train Epoch: 22 [00256/50176 (1%)]\t Loss: 0.250172\n",
      "Train Epoch: 22 [13056/50176 (26%)]\t Loss: 0.174548\n",
      "Train Epoch: 22 [25856/50176 (52%)]\t Loss: 0.254130\n",
      "Train Epoch: 22 [38656/50176 (77%)]\t Loss: 0.322591\n",
      "Test: Average loss: 0.0018, Accuracy: 8561/10000 (86%)\n",
      "================================================================\n",
      "Train Epoch: 23 [00256/50176 (1%)]\t Loss: 0.145239\n",
      "Train Epoch: 23 [13056/50176 (26%)]\t Loss: 0.248516\n",
      "Train Epoch: 23 [25856/50176 (52%)]\t Loss: 0.272081\n",
      "Train Epoch: 23 [38656/50176 (77%)]\t Loss: 0.257226\n",
      "Test: Average loss: 0.0016, Accuracy: 8693/10000 (87%)\n",
      "================================================================\n",
      "Train Epoch: 24 [00256/50176 (1%)]\t Loss: 0.207415\n",
      "Train Epoch: 24 [13056/50176 (26%)]\t Loss: 0.208720\n",
      "Train Epoch: 24 [25856/50176 (52%)]\t Loss: 0.232145\n",
      "Train Epoch: 24 [38656/50176 (77%)]\t Loss: 0.205792\n",
      "Test: Average loss: 0.0018, Accuracy: 8610/10000 (86%)\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "training_mode = \"adv_train\"\n",
    "\n",
    "# Define Model and Launch Training (Define Adversary If You Need It)\n",
    "model = ResNet18().to(device)\n",
    "\n",
    "# Write your code here\n",
    "train(model, train_loader, val_loader, pgd_attack,\n",
    "          mode=training_mode, epochs=epochs, batch_size=batch_size, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
    "          checkpoint_path='model2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a04ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [00256/50176 (1%)]\t Loss: 2.453964\n",
      "Train Epoch: 0 [13056/50176 (26%)]\t Loss: 1.896858\n",
      "Train Epoch: 0 [25856/50176 (52%)]\t Loss: 1.826412\n",
      "Train Epoch: 0 [38656/50176 (77%)]\t Loss: 1.668022\n",
      "Test: Average loss: 0.0089, Accuracy: 3135/10000 (31%)\n",
      "================================================================\n",
      "Train Epoch: 1 [00256/50176 (1%)]\t Loss: 1.484552\n",
      "Train Epoch: 1 [13056/50176 (26%)]\t Loss: 1.525583\n",
      "Train Epoch: 1 [25856/50176 (52%)]\t Loss: 1.541090\n",
      "Train Epoch: 1 [38656/50176 (77%)]\t Loss: 1.391539\n",
      "Test: Average loss: 0.0050, Accuracy: 5566/10000 (56%)\n",
      "================================================================\n",
      "Train Epoch: 2 [00256/50176 (1%)]\t Loss: 1.392904\n",
      "Train Epoch: 2 [13056/50176 (26%)]\t Loss: 1.228774\n",
      "Train Epoch: 2 [25856/50176 (52%)]\t Loss: 1.252674\n",
      "Train Epoch: 2 [38656/50176 (77%)]\t Loss: 1.270619\n",
      "Test: Average loss: 0.0043, Accuracy: 6313/10000 (63%)\n",
      "================================================================\n",
      "Train Epoch: 3 [00256/50176 (1%)]\t Loss: 1.260016\n",
      "Train Epoch: 3 [13056/50176 (26%)]\t Loss: 1.212505\n",
      "Train Epoch: 3 [25856/50176 (52%)]\t Loss: 1.273879\n",
      "Train Epoch: 3 [38656/50176 (77%)]\t Loss: 1.327008\n",
      "Test: Average loss: 0.0043, Accuracy: 6303/10000 (63%)\n",
      "================================================================\n",
      "Train Epoch: 4 [00256/50176 (1%)]\t Loss: 1.197235\n",
      "Train Epoch: 4 [13056/50176 (26%)]\t Loss: 1.187534\n",
      "Train Epoch: 4 [25856/50176 (52%)]\t Loss: 1.141779\n",
      "Train Epoch: 4 [38656/50176 (77%)]\t Loss: 1.048806\n",
      "Test: Average loss: 0.0036, Accuracy: 6918/10000 (69%)\n",
      "================================================================\n",
      "Train Epoch: 5 [00256/50176 (1%)]\t Loss: 1.033896\n",
      "Train Epoch: 5 [13056/50176 (26%)]\t Loss: 1.090545\n",
      "Train Epoch: 5 [25856/50176 (52%)]\t Loss: 0.970399\n",
      "Train Epoch: 5 [38656/50176 (77%)]\t Loss: 1.060664\n",
      "Test: Average loss: 0.0035, Accuracy: 7058/10000 (71%)\n",
      "================================================================\n",
      "Train Epoch: 6 [00256/50176 (1%)]\t Loss: 1.118462\n",
      "Train Epoch: 6 [13056/50176 (26%)]\t Loss: 1.149270\n",
      "Train Epoch: 6 [25856/50176 (52%)]\t Loss: 0.991406\n",
      "Train Epoch: 6 [38656/50176 (77%)]\t Loss: 1.092551\n",
      "Test: Average loss: 0.0032, Accuracy: 7341/10000 (73%)\n",
      "================================================================\n",
      "Train Epoch: 7 [00256/50176 (1%)]\t Loss: 0.901396\n",
      "Train Epoch: 7 [13056/50176 (26%)]\t Loss: 0.997616\n",
      "Train Epoch: 7 [25856/50176 (52%)]\t Loss: 0.997029\n",
      "Train Epoch: 7 [38656/50176 (77%)]\t Loss: 1.091174\n",
      "Test: Average loss: 0.0031, Accuracy: 7543/10000 (75%)\n",
      "================================================================\n",
      "Train Epoch: 8 [00256/50176 (1%)]\t Loss: 0.962087\n",
      "Train Epoch: 8 [13056/50176 (26%)]\t Loss: 0.973354\n",
      "Train Epoch: 8 [25856/50176 (52%)]\t Loss: 1.077522\n",
      "Train Epoch: 8 [38656/50176 (77%)]\t Loss: 0.968993\n",
      "Test: Average loss: 0.0031, Accuracy: 7458/10000 (75%)\n",
      "================================================================\n",
      "Train Epoch: 9 [00256/50176 (1%)]\t Loss: 1.036245\n",
      "Train Epoch: 9 [13056/50176 (26%)]\t Loss: 0.963535\n",
      "Train Epoch: 9 [25856/50176 (52%)]\t Loss: 1.029100\n",
      "Train Epoch: 9 [38656/50176 (77%)]\t Loss: 0.870661\n",
      "Test: Average loss: 0.0029, Accuracy: 7613/10000 (76%)\n",
      "================================================================\n",
      "Train Epoch: 10 [00256/50176 (1%)]\t Loss: 0.921167\n",
      "Train Epoch: 10 [13056/50176 (26%)]\t Loss: 0.840629\n",
      "Train Epoch: 10 [25856/50176 (52%)]\t Loss: 0.887502\n",
      "Train Epoch: 10 [38656/50176 (77%)]\t Loss: 0.905601\n",
      "Test: Average loss: 0.0028, Accuracy: 7804/10000 (78%)\n",
      "================================================================\n",
      "Train Epoch: 11 [00256/50176 (1%)]\t Loss: 0.877230\n",
      "Train Epoch: 11 [13056/50176 (26%)]\t Loss: 0.904333\n",
      "Train Epoch: 11 [25856/50176 (52%)]\t Loss: 0.903250\n",
      "Train Epoch: 11 [38656/50176 (77%)]\t Loss: 0.838812\n",
      "Test: Average loss: 0.0027, Accuracy: 7818/10000 (78%)\n",
      "================================================================\n",
      "Train Epoch: 12 [00256/50176 (1%)]\t Loss: 0.880976\n",
      "Train Epoch: 12 [13056/50176 (26%)]\t Loss: 0.897966\n",
      "Train Epoch: 12 [25856/50176 (52%)]\t Loss: 0.892058\n",
      "Train Epoch: 12 [38656/50176 (77%)]\t Loss: 0.856082\n",
      "Test: Average loss: 0.0025, Accuracy: 7976/10000 (80%)\n",
      "================================================================\n",
      "Train Epoch: 13 [00256/50176 (1%)]\t Loss: 0.829795\n",
      "Train Epoch: 13 [13056/50176 (26%)]\t Loss: 0.822142\n",
      "Train Epoch: 13 [25856/50176 (52%)]\t Loss: 0.828753\n",
      "Train Epoch: 13 [38656/50176 (77%)]\t Loss: 0.876609\n",
      "Test: Average loss: 0.0027, Accuracy: 7907/10000 (79%)\n",
      "================================================================\n",
      "Train Epoch: 14 [00256/50176 (1%)]\t Loss: 0.845965\n",
      "Train Epoch: 14 [13056/50176 (26%)]\t Loss: 0.901798\n",
      "Train Epoch: 14 [25856/50176 (52%)]\t Loss: 0.880084\n",
      "Train Epoch: 14 [38656/50176 (77%)]\t Loss: 0.841480\n",
      "Test: Average loss: 0.0026, Accuracy: 8024/10000 (80%)\n",
      "================================================================\n",
      "Train Epoch: 15 [00256/50176 (1%)]\t Loss: 0.837461\n",
      "Train Epoch: 15 [13056/50176 (26%)]\t Loss: 0.802546\n",
      "Train Epoch: 15 [25856/50176 (52%)]\t Loss: 0.795327\n",
      "Train Epoch: 15 [38656/50176 (77%)]\t Loss: 0.793136\n",
      "Test: Average loss: 0.0027, Accuracy: 7872/10000 (79%)\n",
      "================================================================\n",
      "Train Epoch: 16 [00256/50176 (1%)]\t Loss: 0.831719\n",
      "Train Epoch: 16 [13056/50176 (26%)]\t Loss: 0.815635\n",
      "Train Epoch: 16 [25856/50176 (52%)]\t Loss: 0.769069\n",
      "Train Epoch: 16 [38656/50176 (77%)]\t Loss: 0.756805\n",
      "Test: Average loss: 0.0026, Accuracy: 8063/10000 (81%)\n",
      "================================================================\n",
      "Train Epoch: 17 [00256/50176 (1%)]\t Loss: 0.823159\n",
      "Train Epoch: 17 [13056/50176 (26%)]\t Loss: 0.773279\n",
      "Train Epoch: 17 [25856/50176 (52%)]\t Loss: 0.781576\n",
      "Train Epoch: 17 [38656/50176 (77%)]\t Loss: 0.789048\n",
      "Test: Average loss: 0.0026, Accuracy: 7983/10000 (80%)\n",
      "================================================================\n",
      "Train Epoch: 18 [00256/50176 (1%)]\t Loss: 0.730135\n",
      "Train Epoch: 18 [13056/50176 (26%)]\t Loss: 0.834338\n",
      "Train Epoch: 18 [25856/50176 (52%)]\t Loss: 0.801700\n",
      "Train Epoch: 18 [38656/50176 (77%)]\t Loss: 0.727956\n",
      "Test: Average loss: 0.0024, Accuracy: 8175/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 19 [00256/50176 (1%)]\t Loss: 0.788984\n",
      "Train Epoch: 19 [13056/50176 (26%)]\t Loss: 0.674993\n",
      "Train Epoch: 19 [25856/50176 (52%)]\t Loss: 0.718767\n",
      "Train Epoch: 19 [38656/50176 (77%)]\t Loss: 0.764821\n",
      "Test: Average loss: 0.0024, Accuracy: 8187/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 20 [00256/50176 (1%)]\t Loss: 0.747384\n",
      "Train Epoch: 20 [13056/50176 (26%)]\t Loss: 0.649852\n",
      "Train Epoch: 20 [25856/50176 (52%)]\t Loss: 0.703622\n",
      "Train Epoch: 20 [38656/50176 (77%)]\t Loss: 0.677258\n",
      "Test: Average loss: 0.0025, Accuracy: 8110/10000 (81%)\n",
      "================================================================\n",
      "Train Epoch: 21 [00256/50176 (1%)]\t Loss: 0.824177\n",
      "Train Epoch: 21 [13056/50176 (26%)]\t Loss: 0.667752\n",
      "Train Epoch: 21 [25856/50176 (52%)]\t Loss: 0.702685\n",
      "Train Epoch: 21 [38656/50176 (77%)]\t Loss: 0.724360\n",
      "Test: Average loss: 0.0023, Accuracy: 8327/10000 (83%)\n",
      "================================================================\n",
      "Train Epoch: 22 [00256/50176 (1%)]\t Loss: 0.637707\n",
      "Train Epoch: 22 [13056/50176 (26%)]\t Loss: 0.663692\n",
      "Train Epoch: 22 [25856/50176 (52%)]\t Loss: 0.712708\n",
      "Train Epoch: 22 [38656/50176 (77%)]\t Loss: 0.667338\n",
      "Test: Average loss: 0.0023, Accuracy: 8206/10000 (82%)\n",
      "================================================================\n",
      "Train Epoch: 23 [00256/50176 (1%)]\t Loss: 0.673791\n",
      "Train Epoch: 23 [13056/50176 (26%)]\t Loss: 0.682261\n",
      "Train Epoch: 23 [25856/50176 (52%)]\t Loss: 0.715549\n",
      "Train Epoch: 23 [38656/50176 (77%)]\t Loss: 0.659981\n",
      "Test: Average loss: 0.0026, Accuracy: 8066/10000 (81%)\n",
      "================================================================\n",
      "Train Epoch: 24 [00256/50176 (1%)]\t Loss: 0.682962\n",
      "Train Epoch: 24 [13056/50176 (26%)]\t Loss: 0.700586\n",
      "Train Epoch: 24 [25856/50176 (52%)]\t Loss: 0.654806\n",
      "Train Epoch: 24 [38656/50176 (77%)]\t Loss: 0.640500\n",
      "Test: Average loss: 0.0025, Accuracy: 8253/10000 (83%)\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "training_mode = \"adv_train_trades\"\n",
    "\n",
    "# Define Model and Launch Training (Define Adversary If You Need It)\n",
    "model = ResNet18().to(device)\n",
    "\n",
    "# Write your code here\n",
    "train(model, train_loader, val_loader, pgd_attack,\n",
    "          mode=training_mode, epochs=epochs, batch_size=batch_size, learning_rate=0.05, momentum=0.9, weight_decay=2e-4,\n",
    "          checkpoint_path='model3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5036a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [00256/50176 (1%)]\t Loss: 2.415612\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet18()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Write your code here\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgd_attack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel4.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, pgd_attack, mode, epochs, batch_size, learning_rate, momentum, weight_decay, checkpoint_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mtrain_ep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgd_attack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# evaluate clean accuracy\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m eval_test(model, val_loader)\n",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36mtrain_ep\u001b[1;34m(model, train_loader, mode, pgd_attack, optimizer, criterion, epoch, batch_size)\u001b[0m\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     25\u001b[0m benign_inputs, benign_targets_a, benign_targets_b, benign_lam \u001b[38;5;241m=\u001b[39m mixup_data(inputs, targets)            \n\u001b[1;32m---> 26\u001b[0m adv_x \u001b[38;5;241m=\u001b[39m \u001b[43mpgd_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m adv_inputs, adv_targets_a, adv_targets_b, adv_lam \u001b[38;5;241m=\u001b[39m mixup_data(adv_x, targets)\n\u001b[0;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 37\u001b[0m, in \u001b[0;36mLinfPGDAttack.forward\u001b[1;34m(self, x_natural, y)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_natural, y):\n\u001b[1;32m---> 37\u001b[0m     x_adv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_natural\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_adv\n",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36mLinfPGDAttack.perturb\u001b[1;34m(self, x_natural, y)\u001b[0m\n\u001b[0;32m     24\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x_adv)\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(pred_y, y)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     28\u001b[0m     x_adv \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_size \u001b[38;5;241m*\u001b[39m x_adv\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39msign()\n",
      "File \u001b[1;32mc:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\torch\\autograd\\graph.py:681\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    679\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 681\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_mode = \"adv_train_mixup\"\n",
    "\n",
    "# Define Model and Launch Training (Define Adversary If You Need It)\n",
    "model = ResNet18().to(device)\n",
    "\n",
    "# Write your code here\n",
    "train(model, train_loader, val_loader, pgd_attack,\n",
    "          mode=training_mode, epochs=epochs, batch_size=batch_size, learning_rate=0.1, momentum=0.9, weight_decay=2e-4,\n",
    "          checkpoint_path='model4.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a009caec",
   "metadata": {},
   "source": [
    "### Q3 (20 points)\n",
    "Use eval_robust() to evaluate each model's robustness against LinfPGDAttack()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d8a3ad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet18()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel1.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m robust_loss, robust_acc \u001b[38;5;241m=\u001b[39m \u001b[43meval_robust\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgd_attack\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Mode: Natural, robust_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrobust_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, robust_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrobust_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# *********** Your code ends here *************\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\silvr\\OneDrive\\Desktop\\deep-learning-generalization-and-robustness\\hw2\\utils.py:61\u001b[0m, in \u001b[0;36meval_robust\u001b[1;34m(model, test_loader, pgd_attack)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     60\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 61\u001b[0m     adv \u001b[38;5;241m=\u001b[39m \u001b[43mpgd_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(adv)\n\u001b[0;32m     63\u001b[0m     robust_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(outputs, targets)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m, in \u001b[0;36mLinfPGDAttack.forward\u001b[1;34m(self, x_natural, y)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_natural, y):\n\u001b[1;32m---> 41\u001b[0m     x_adv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_natural\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_adv\n",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36mLinfPGDAttack.perturb\u001b[1;34m(self, x_natural, y)\u001b[0m\n\u001b[0;32m     24\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x_adv)\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(pred_y, y)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     29\u001b[0m     x_adv_update \u001b[38;5;241m=\u001b[39m x_adv \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_size \u001b[38;5;241m*\u001b[39m x_adv\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39msign()\n",
      "File \u001b[1;32mc:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\silvr\\anaconda3\\envs\\Diffuser\\Lib\\site-packages\\torch\\autograd\\graph.py:681\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    679\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 681\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from utils import eval_robust\n",
    "\n",
    "# *********** Your code starts here ***********\n",
    "model = ResNet18().to(device)\n",
    "model.load_state_dict(torch.load('model1.pt'))\n",
    "robust_loss, robust_acc = eval_robust(model, val_loader, pgd_attack)\n",
    "print(f'Training Mode: Natural, robust_loss: {robust_loss}, robust_acc: {robust_acc}')\n",
    "# *********** Your code ends here *************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********** Your code starts here ***********\n",
    "model = ResNet18().to(device)\n",
    "model.load_state_dict(torch.load('model2.pt'))\n",
    "robust_loss, robust_acc = eval_robust(model, val_loader, pgd_attack)\n",
    "print(f'Training Mode: adv_train, robust_loss: {robust_loss}, robust_acc: {robust_acc}')\n",
    "# *********** Your code ends here *************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56863557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********** Your code starts here ***********\n",
    "model = ResNet18().to(device)\n",
    "model.load_state_dict(torch.load('model3.pt'))\n",
    "robust_loss, robust_acc = eval_robust(model, val_loader, pgd_attack)\n",
    "print(f'Training Mode: adv_train_trades, robust_loss: {robust_loss}, robust_acc: {robust_acc}')\n",
    "# *********** Your code ends here *************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********** Your code starts here ***********\n",
    "model = ResNet18().to(device)\n",
    "model.load_state_dict(torch.load('model4.pt'))\n",
    "robust_loss, robust_acc = eval_robust(model, val_loader, pgd_attack)\n",
    "print(f'Training Mode: adv_train_mixup, robust_loss: {robust_loss}, robust_acc: {robust_acc}')\n",
    "# *********** Your code ends here *************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60a88c",
   "metadata": {},
   "source": [
    "### Q4 (10 points)\n",
    "Visualize 10 adversarial examples from (with ground truth labels and model's predictions) from the model having best robust accuarcy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********** Your code starts here ***********\n",
    "\n",
    "\n",
    "\n",
    "# *********** Your code ends here *************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6dfa2",
   "metadata": {},
   "source": [
    "### Q5 (10 points)\n",
    "Which adversarial training algorithm achieves the best robust accuracy in only 25 training epochs? Why do you think that adversarial training algorithm outperforms others? Do you encounter any difficulties when you implement this assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15618d3",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412124b3",
   "metadata": {},
   "source": [
    "## Submission Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9ffc6",
   "metadata": {},
   "source": [
    "Please submit this  to Canvas, as well as results, as per instructions. \n",
    "Please compress `First_Middle_Last_HW1/` into one zip file with the name `First_Middle_Last_HW1.zip` before uploading it to Canvas. The directory contains your notebook and four model checkpoints. As listed below:\n",
    "\n",
    "- $\\texttt{Assignment_2.ipynb}$: your code\n",
    "- $\\texttt{model1.pt}$: your model checkpoint with *natural training* \n",
    "- $\\texttt{model2.pt}$: your model checkpoint with *adv_train* training\n",
    "- $\\texttt{model3.pt}$: your model checkpoint with *adv_train_trades* training\n",
    "- $\\texttt{model4.pt}$: your model checkpoint with *adv_train_mixup* training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec7b76",
   "metadata": {},
   "source": [
    "## Academic Integrity\n",
    "\n",
    "This homework assignment must be done individually. Sharing code or model specifications is strictly prohibited. Homework discussions are allowed only on Piazza, according to the policy outlined on the course web page: [https://canvas.dartmouth.edu/courses/63219](https://canvas.dartmouth.edu/courses/63219). You are not allowed to search online for auxiliary software, reference models, architecture specifications, or additional data to solve the homework assignment. Your submission must be entirely your own work. That is, the code and the answers that you submit must be created, typed, and documented by you alone, based exclusively on the materials discussed in class, and released with the homework assignment. You can obviously consult the class slides posted in Canvas, your lecture notes, and the textbook. Important: the models you will submit for this homework assignment must be\n",
    "trained exclusively on the specified data provided with this assignment. You can, of course, play with other datasets in your spare time. These rules will be strictly enforced, and any violation will be treated seriously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
